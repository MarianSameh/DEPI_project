{
    "name": "Spark Transform",
    "properties": {
        "nbformat": 4,
        "nbformat_minor": 2,
        "bigDataPool": {
            "referenceName": "sparkes",
            "type": "BigDataPoolReference"
        },
        "sessionProperties": {
            "driverMemory": "28g",
            "driverCores": 4,
            "executorMemory": "28g",
            "executorCores": 4,
            "numExecutors": 2,
            "runAsWorkspaceSystemIdentity": false,
            "conf": {
                "spark.dynamicAllocation.enabled": "false",
                "spark.dynamicAllocation.minExecutors": "2",
                "spark.dynamicAllocation.maxExecutors": "2",
                "spark.autotune.trackingId": "961efddd-f120-41ce-b3ee-d0777cc8267e"
            }
        },
        "metadata": {
            "saveOutput": true,
            "synapse_widget": {
                "version": "0.1"
            },
            "enableDebugMode": false,
            "language_info": {
                "name": "python"
            },
            "a365ComputeOptions": {
                "id": "/subscriptions/75914440-c765-410d-8702-035e4e5e806e/resourceGroups/spark/providers/Microsoft.Synapse/workspaces/synapz/bigDataPools/sparkes",
                "name": "sparkes",
                "type": "Spark",
                "endpoint": "https://synapz.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkes",
                "auth": {
                    "type": "AAD",
                    "authResource": "https://dev.azuresynapse.net",
                    "authHeader": null
                },
                "sparkVersion": "3.4",
                "nodeCount": 3,
                "cores": 4,
                "memory": 28,
                "extraHeader": null
            },
            "sessionKeepAliveTimeout": 30
        },
        "cells": [
            {
                "cell_type": "markdown",
                "source": [
                    "# Transform data by using Spark\n",
                    "\n",
                    "This notebook transforms sales order data; converting it from CSV to Parquet format and splitting customer name into two separate fields.\n",
                    "\n",
                    "## Set variables"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    },
                    "tags": [
                        "parameters"
                    ]
                },
                "source": [
                    "import uuid\r\n",
                    "\r\n",
                    "# Variable for unique folder name\r\n",
                    "folderName = uuid.uuid4()"
                ],
                "outputs": []
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Load source data\r\n",
                    "\r\n",
                    "Let's start by loading some historical sales order data into a dataframe."
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "collapsed": false
                },
                "source": [
                    "order_details = spark.read.csv('/data/*.csv', header=True, inferSchema=True)"
                ],
                "outputs": []
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Transform the data structure\r\n",
                    "\r\n",
                    "The source data includes a **CustomerName** field, that contains the customer's first and last name. Modify the dataframe to separate this field into separate **FirstName** and **LastName** fields."
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    },
                    "collapsed": false
                },
                "source": [
                    "from pyspark.sql.functions import split, col\r\n",
                    "\r\n",
                    "# Create the new FirstName and LastName fields\r\n",
                    "transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\r\n",
                    "\r\n",
                    "# Remove the CustomerName field\r\n",
                    "transformed_df = transformed_df.drop(\"CustomerName\")"
                ],
                "outputs": []
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Save the transformed data\r\n",
                    "\r\n",
                    "Now save the transformed dataframe in Parquet format in a folder specified in a variable (Overwriting the data if it already exists)."
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "transformed_df.write.mode(\"overwrite\").parquet('/%s' % folderName)\r\n",
                    "print (\"Transformed data saved in %s!\" % folderName)"
                ],
                "outputs": []
            }
        ]
    }
}